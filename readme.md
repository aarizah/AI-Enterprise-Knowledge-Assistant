# Case Study ‚Äì AI Enterprise Knowledge Assistant (EKA)

## 1. Resumen en una l√≠nea

Un asistente tipo *ChatGPT con datos internos de la empresa*, que responde con **citas verificables** y puede **ejecutar acciones** (crear tickets, enviar reportes, actualizar CRM) directamente desde Slack o web.

---

## 2. Problema

- El conocimiento interno est√° disperso en **PDFs, wikis, Google Drive, Notion**.
- Los empleados pierden **4‚Äì6 horas/semana** buscando informaci√≥n o preguntando a otros.
- El soporte interno (IT, HR, Legal) est√° **saturado**, generando cuellos de botella.
- Los chatbots tradicionales **alucinan** o no integran acciones, creando desconfianza.

**Impacto negativo:** tiempo perdido, decisiones lentas, soporte colapsado, riesgo reputacional por respuestas err√≥neas.

---

## 3. Soluci√≥n

El **EKA** conecta todo el conocimiento interno, responde con **citas verificables** y ejecuta **acciones reales** en los sistemas de la empresa.

- **Conexi√≥n a fuentes**: PDFs, Google Drive, Confluence, Notion.
- **Respuestas con citaci√≥n**: si no encuentra evidencia, responde ‚Äúno answer‚Äù.
- **Acciones directas**: crear ticket en Jira, enviar email, actualizar HubSpot.
- **Seguridad corporativa**: RBAC (roles), SSO con Google/Slack, aislamiento de datos.
- **Observabilidad**: m√©tricas de uso, latencia, coste por consulta, ratio de ‚Äúno answer‚Äù.

---

## 4. Diferenciadores clave

- **No es un ‚Äúchat con PDFs‚Äù**:
    - Respuestas siempre citadas y auditables.
    - Si no hay evidencia, no inventa.
- **Acciona, no solo responde**: genera tickets, emails o actualiza CRMs.
- **Pensado como micro-SaaS**: multi-tenant, auth b√°sica, aislamiento de datos.
- **Enfoque de producto**: instalaci√≥n en 5 minutos, UX simple, onboarding guiado.
- **Reliability y coste**: multi-model routing, caching, fallback para latencia y ahorro.

---

## 5. Arquitectura de alto nivel

**Ingesta** (connectors: PDF/Drive) ‚Üí **Prepro** (chunking + metadata) ‚Üí **Vector DB (Postgres/pgvector)** ‚Üí **Orquestaci√≥n** (LangChain/LlamaIndex + tools) ‚Üí **LLM** ‚Üí **Guardrails** (citaci√≥n/PII/no answer) ‚Üí **Acciones** (APIs: Jira/Gmail/HubSpot) ‚Üí **Telemetr√≠a** (logs, latencia, coste).

---

## 6. MVP funcional (lo que mostr√©)

- Subir documentos (PDF/Google Drive).
- Chat con citaci√≥n de fuente.
- Bot√≥n de acci√≥n: ‚Äúcrear ticket en Jira‚Äù o ‚Äúenviar email‚Äù.
- Panel admin: gesti√≥n de datasets, roles y m√©tricas b√°sicas.
- Deploy p√∫blico con login Google/Slack.

---

## 7. M√©tricas instrumentadas

- **Calidad:** % respuestas con cita, ratio ‚Äúno answer‚Äù.
- **Rendimiento:** tiempo medio a primera respuesta, p95 < 2s.
- **Coste:** $/100 consultas, % ahorro con caching.
- **Seguridad:** PII detectada y redacci√≥n autom√°tica.
- **Adopci√≥n:** consultas/d√≠a, usuarios activos, CSAT b√°sico (üëçüëé).

---

## 8. Resultados (con datos simulados)

- **40% menos tiempo** de b√∫squeda interna.
- **30% menos tickets** a soporte de IT/HR.
- **92% de respuestas con citaci√≥n verificable**.
- **35% de reducci√≥n en coste por consulta** usando multi-model routing + caching.

---

## 9. Riesgos y trade-offs

- **Costo LLM** ‚Üí mitigado con caching y modelos m√°s baratos para queries simples.
- **Privacidad** ‚Üí opci√≥n de self-host on-prem (docker-compose).
- **UX adoption** ‚Üí onboarding guiado, bot√≥n de feedback (‚Äúüëç/üëé‚Äù) para ganar confianza.

---

## 10. Pitch de 20 segundos (para entrevistas)

> ‚ÄúConstru√≠ un asistente de IA que conecta el conocimiento interno de una empresa, responde solo con citas verificables y puede ejecutar acciones en los sistemas. Se instala en Slack en 5 minutos y reduce en m√°s de 30% el tiempo que empleados gastan buscando informaci√≥n.‚Äù
> 

---

## 11. Futuro (si lo llevara m√°s all√° del portafolio)

- Integraci√≥n con m√°s SaaS (Confluence, Salesforce).
- Dashboard de analytics por equipo/√°rea.
- Autoservicio: los usuarios conectan sus propias fuentes.
- Fine-tuning ligero con feedback interno.

---

## Modo desarrollo r√°pido (DB en Docker)

Para iterar el backend y frontend con hot-reload sin reconstruir contenedores, levanta solo la base de datos con Docker y corre frontend/backend localmente.

1) Levantar solo la base de datos (Docker):

```powershell
docker compose up -d db
```

2) Backend (FastAPI) en local con recarga:

- Copia `backend/.env.local.example` a `backend/.env.local` y ajusta `DATABASE_URL` para apuntar a `localhost` y el puerto mapeado de Postgres (por defecto 5432).
- Crea y activa un entorno Python 3.12 y instala dependencias:

```powershell
cd backend
python -m venv .venv; .\.venv\Scripts\Activate.ps1
pip install -e .
uvicorn app.main:app --host 127.0.0.1 --port 8000 --reload
```

3) Frontend (Next.js) en local con recarga:

- Copia `frontend/.env.local.example` a `frontend/.env.local` (apunta a `http://localhost:8000`).
- Instala deps y corre dev server:

```powershell
cd frontend
pnpm install
pnpm dev
```

Notas:
- `docker compose up -d db` levanta solo Postgres; no es necesario iniciar `frontend`/`backend` en Docker para desarrollo.
- El backend ahora toma `BACKEND_PORT` (por defecto 8000) y no colisiona con Next (3000).
- El backend carga `.env.local` si existe; de lo contrario, usa `.env` (√∫til para separar Docker vs local).